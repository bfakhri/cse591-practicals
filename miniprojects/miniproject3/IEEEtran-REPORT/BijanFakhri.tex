

\documentclass[conference,compsoc]{IEEEtran}



% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi




% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
\title{CSE 591 - Introduction to Deep Learning\\Mini Project 3}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Bijan Fakhri}
\IEEEauthorblockA{School of Computing, Informatics and\\Decision Systems Engineering\\
Arizona State University\\
Tempe, Arizona 85044\\
Email: bfakhri@asu.edu}}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Hyperparameter tuning is an important aspect of deep learning (or any machine learning in general). The goal of Mini Project 3 is to become familiar with the Theano toolbox Yann by tuning the hyperparameters of the Yann MLNN tutorial. Hyperparameters were heuristically ordered from greatest to least impactful (in terms of their impact on error rate). The error rate of my tuned network was 98.53\%.
\end{abstract}



\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
The parameters in question were regularization, optimization technique, momentum technique, and the learning rate. Below is a table of tunable hyperparameters, their options and values, under the scope of this mini project. 

\renewcommand{\arraystretch}{1.1}
\begin{center}
  \begin{tabular}{ | c | c  | c | }
    \hline
    \textbf{Hyperparameter} & \textbf{Options} & \textbf{Values} \\ \hline
    Regularization & ON & L1 Coeff \\  & OFF & L2 Coeff \\ \hline
    Optimization & RMSProp & \_ \\ & AdaGrad & \\ \hline
    Momentum &  None & StartVal \\ &  Polyak &  EndVal \\ & Nesterov &  EndEpoch \\ \hline
    Learning Rate &   & AnnealingFactor \\ & -- &  FirstEraRate \\ & &  SecondEraRate \\ \hline
  \end{tabular}
\end{center}

 After succesfully installing Yann, the tutorial was run using the default values. The default values gave very good results at \textbf{98.39\%}. The network was then purged of tuned hyperparameters, making the network as simple as possible, creating a good baseline to work with. Training/testing the clean network resulted in an accuracy of \textbf{97.98\%}. This became the baseline. A table of the state of this network is below. 
 
 \renewcommand{\arraystretch}{1.1}
 \begin{center}
   \begin{tabular}{ | c | c | c | }
     \hline
     \textbf{Hyperparameter} & \textbf{Options} & \textbf{Values} \\ \hline
     Regularization & OFF & -- \\ \hline
     Optimization & RMSProp & -- \\ \hline
     Momentum &  None &  -- \\ \hline
     Learning Rate & -- & (0.05, 0.01, 0.001)  \\ \hline
   \end{tabular}
 \end{center}


\section{Tuning}
%
Based on intuition, I ranked the hyperparameters in order of what I though would have the largest effect:
\textit{Regularization, Optimization, Momentum, LearningRate}. Begining with regularization, turning it on and with the default parameters (0.001, 0.001), accuracy was affected very slightly negatively to \textbf{97.97\%}. Raising the L1 and L2 coefficient to 0.01 and 0.02 raised accuracy to \textbf{98.18\%}. Deciding to keep this, I moved onto \textit{Optimization}. Changing it to textit{Adagrad} had the negative effect of reducing accuracy to \textbf{98.11\%}. No further exploration of optimization technique was deemed necessary. Next hyperparameter to tune was \textit{Momentum}. Adding \textit{Momentum} with values (0.5, 0.95, 30) resulted in poor gains. Raising the startVal to 0.9 ((0.9, 0.95, 30) resulted in the greatest improvement in accuracy: Polyak \textit{Momentum} resulted in an accuracy of \textbf{98.53\%}. Trying Nesterov \textit{Momentum} gave comparable but less stable (changed each iteration) results. For this reason, Polyak was chosen going forward. Unfortunately, with \textit{LearningRate}, deviating from the default values of (0.1,0.001,0.005) only resulted in a reduction in accuracy. The final state of the network (best local minima) is listed below: 
\\
 \renewcommand{\arraystretch}{1.1}
 \begin{center}
   \begin{tabular}{ | c | c | c | }
     \hline
     \textbf{Hyperparameter} & \textbf{Options} & \textbf{Values} \\ \hline
     Regularization & ON & (0.01, 0.02) \\ \hline
     Optimization & RMSProp & -- \\ \hline
     Momentum &  Polyak &  (0.9, 0.95, 30)-- \\ \hline
     Learning Rate & -- & (0.05, 0.01, 0.001)  \\ \hline
   \end{tabular}
 \end{center}


\section{Conclusion}
%
In conclusion, the hyperparameters on the Yann MLNN tutorial were highly tuned from the start: the error rate of the tuned network was \textbf{98.39\%}. Because of this, there was not a lot of room for improvement. Starting from a more reasonable baseline (stripping the network of tuned parameters to learn the tuning manually) resulted in a tuning gain of \textbf{98.53\%} - \textbf{97.98\%} = \textbf{0.55 \%}. 



% that's all folks
\end{document}


